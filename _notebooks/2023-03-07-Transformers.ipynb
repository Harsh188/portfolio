{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "> Building the original Transformers (Vaswani et al) architecture from scratch.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "In this notebook I'll be constructing the Transformers architecture from scratch using the PyTorch module. \n",
    "\n",
    "The Transformer (Vaswani et al) was originally proposed in 2018 by a group of Google researchers. Their paper titled \"Attention is all you need\" tackled sequence modeling problems by adopting the attention mechanism to draw global dependencies between input and output. The Transformer architecture was the first of its kind to be entirely dependent on the self-attention module while disposing the notion of RNNs and convolution.\n",
    "\n",
    "The code developed in this notebook is extracted from [Transformer from scratch using pytorch](https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-03-07/TransformerArch.png)\n",
    "Image credits: Vaswani et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Unlike RNNs or LSTMs, the Transformer processes all of the inputs parallely. This makes the architecture a lot more efficient however the positional information is lost during the process. Therefore we must encode these values into the input.\n",
    "\n",
    "The authors of the paper use the following functions to create positional encoding: \n",
    "\n",
    "$$ PE_{(pos, 2i)} = \\sin({pos/10000^{2i/d_{model}}}) $$\n",
    "$$ PE_{(pos, 2i+1)} = \\cos({pos/10000^{2i/d_{model}}}) $$\n",
    "\n",
    "where:\n",
    "\n",
    "`pos` = position in the input vector\n",
    "\n",
    "`i` = position of the embedding vector\n",
    "\n",
    "`d_model` = dimension of the embedding vector\n",
    "\n",
    "On odd time steps a cosine function was used and on even the sine function was used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positional encoding as a pytorch module\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,seq_len,embd_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # initialize the PE vector\n",
    "        pe_tensor = torch.zeros(size=(seq_len,embd_len))\n",
    "        \n",
    "        # Set the values according to the functions\n",
    "        for pos in range(0,seq_len):\n",
    "            for i in range(0,embd_len,2):\n",
    "                # Even\n",
    "                pe_tensor[pos,i] = math.sin(pos/(10000 ** ((2*i)/embd_len)))\n",
    "                # Odd\n",
    "                pe_tensor[pos,i+1] = math.cos(pos/(10000 ** ((2*i)/embd_len)))\n",
    "        \n",
    "        self.register_buffer('pe',pe_tensor)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + torch.autograd.Variable(self.pe, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "The paper uses Multiheaded attention which is a combination of multiple self attention heads.\n",
    "\n",
    "The self attention takes in key, query and value as input. These matrixes are learned during training. The figure below illustrates the mathematical operations that make up the scaled dot-product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-03-07/MHArch.png)\n",
    "Image credits: Vaswani et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above we can decompose the Multi-Head Attention into 4 steps:\n",
    "1. Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop the MultiHeadAttention module which applies the Scaled Dot-Product Attention to\n",
    "# a matrix of Query, Key Values.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            n_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim    #512 dim\n",
    "        self.n_heads = n_heads   #8\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d\n",
    "       \n",
    "        #key,query and value matrixes    #64 x 64   \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "\n",
    "    def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key : key vector\n",
    "           query : query vector\n",
    "           value : value vector\n",
    "           mask: mask for decoder\n",
    "        \n",
    "        Returns:\n",
    "           output vector from multihead attention\n",
    "        \"\"\"\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        \n",
    "        # query dimension can change in decoder during inference. \n",
    "        # so we cant take general seq_length\n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        # 32x10x512\n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "       \n",
    "        k = self.key_matrix(key)       # (32x10x8x64)\n",
    "        q = self.query_matrix(query)   \n",
    "        v = self.value_matrix(value)\n",
    "\n",
    "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n",
    "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "       \n",
    "        # computes attention\n",
    "        # adjust key for matrix multiplication\n",
    "        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)\n",
    "        product = torch.matmul(q, k_adjusted)  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)\n",
    "      \n",
    "        \n",
    "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
    "        if mask is not None:\n",
    "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        #divising by square root of key dimension\n",
    "        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)\n",
    "\n",
    "        #applying softmax\n",
    "        scores = F.softmax(product, dim=-1)\n",
    " \n",
    "        #mutiply with value matrix\n",
    "        scores = torch.matmul(scores, v)  ##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) \n",
    "        \n",
    "        #concatenated output\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n",
    "        \n",
    "        output = self.out(concat) #(32,10,512) -> (32,10,512)\n",
    "       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "From the images shown above, we can observe that the Transformer architecture is comprised of two primary blocks known as the encoder and decoder. The following code sets up the encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim) \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,key,query,value):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           norm2_out: output of transformer block\n",
    "        \n",
    "        \"\"\"\n",
    "        attention_out = self.attention(key,query,value)  #32x10x512\n",
    "        attention_residual_out = attention_out + value  #32x10x512\n",
    "        norm1_out = self.dropout1(self.norm1(attention_residual_out)) #32x10x512\n",
    "\n",
    "        feed_fwd_out = self.feed_forward(norm1_out) #32x10x512 -> #32x10x2048 -> 32x10x512\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out #32x10x512\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) #32x10x512\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PositionalEncoding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out,out,out)\n",
    "\n",
    "        return out  #32x10x512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
    "\n",
    "    def forward(self, key, query, x,mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           out: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        #we need to pass mask mask only to fst attention\n",
    "        attention = self.attention(x,x,x,mask=mask) #32x10x512\n",
    "        value = self.dropout(self.norm(attention + x))\n",
    "        \n",
    "        out = self.transformer_block(key, query, value)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           target_vocab_size: vocabulary size of taget\n",
    "           embed_dim: dimension of embedding\n",
    "           seq_len : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEncoding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector from target\n",
    "            enc_out : output from encoder layer\n",
    "            trg_mask: mask for decoder self attention\n",
    "        Returns:\n",
    "            out: output vector\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        x = self.word_embedding(x)  #32x10x512\n",
    "        x = self.position_embedding(x) #32x10x512\n",
    "        x = self.dropout(x)\n",
    "     \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask) \n",
    "\n",
    "        out = F.softmax(self.fc_out(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           embed_dim:  dimension of embedding \n",
    "           src_vocab_size: vocabulary size of source\n",
    "           target_vocab_size: vocabulary size of target\n",
    "           seq_length : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask    \n",
    "\n",
    "    def decode(self,src,trg):\n",
    "        \"\"\"\n",
    "        for inference\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out_labels : returns final prediction of sequence\n",
    "        \"\"\"\n",
    "\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
    "        #outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)\n",
    "        out = trg\n",
    "\n",
    "        for i in range(seq_len): #10\n",
    "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
    "            # taking the last token\n",
    "            out = out[:,-1,:]\n",
    "\n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out,axis=0)\n",
    "\n",
    "        return out_labels\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out: final vector which returns probabilities of each target word\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "   \n",
    "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Translation\n",
    "In the paper, the authors show the effectivness of the Transformer architecture using Machine Translation as their primary task. They also show that the model is generalizable by evaluating it on English Constituency Parsing. To keep it simple, in the following code block I've defined hardcoded tensors as inputs and target outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12]) torch.Size([2, 12])\n",
      "torch.Size([2, 12, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/2294999219.py:83: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(self.fc_out(x))\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length= 12\n",
    "\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n",
    "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "\n",
    "print(src.shape,target.shape)\n",
    "model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n",
    "                    target_vocab_size=target_vocab_size, seq_length=seq_length,\n",
    "                    num_layers=num_layers, expansion_factor=4, n_heads=8)\n",
    "\n",
    "out = model(src, target)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
